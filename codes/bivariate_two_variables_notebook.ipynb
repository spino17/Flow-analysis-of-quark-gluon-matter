{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import pandas\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Used in the original paper\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    nume = (y_pred - y_true) ** 2\n",
    "    # print(nume)\n",
    "    # deno = y_true\n",
    "    return torch.sum(nume) / torch.sum(y_true)\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    return -1 * torch.sum(y_true * torch.log(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(X, y, batch_size, validation_split=0.2):\n",
    "    X = torch.from_numpy(X).float()\n",
    "    y = torch.from_numpy(y).float()\n",
    "    N_total = torch.sum(y)\n",
    "    dataset = TensorDataset(X, y)\n",
    "    batch_size = batch_size\n",
    "    training_length = int(len(dataset) * (1 - validation_split))\n",
    "    lengths = [training_length, len(dataset) - training_length]\n",
    "    train_dataset, validation_dataset = random_split(dataset, lengths)\n",
    "    return (\n",
    "        DataLoader(train_dataset, batch_size),\n",
    "        DataLoader(validation_dataset, batch_size),\n",
    "        N_total,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    train_loader, val_loader, N_total, num_dim, num_epochs, num_steps, learning_rate\n",
    "):\n",
    "    \"\"\"\n",
    "    The dataset contains 2-D tensor where first dimension runs along batch and\n",
    "    second dimension runs along component of an indiviual data point. The y\n",
    "    contains the true value of histogram.\n",
    "\n",
    "    The optimization algorithm used here is Stochastic Gradient Descent (SGD)\n",
    "    and data is batched according to the batch_size. The gradients calculated\n",
    "    at each epoch are thus a stochastic estimate of the gradient over the com-\n",
    "    plete dataset.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # defining parameters of the model\n",
    "    mean_coeff = torch.tensor([[2.6946, 1.9212303]], requires_grad=True)  # 2-D (N, E)\n",
    "    lower_coeff = torch.tensor(\n",
    "        [1.3196, 0.9278, 0.2146], requires_grad=True\n",
    "    )  # vectorized lower triangular matrix\n",
    "    # initial values of a and b are sampled from normal distribution with mean zero and covariance matrix one\n",
    "    a = torch.tensor([[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]], requires_grad=True)\n",
    "    b = torch.tensor([[0.0], [0.0], [0.0]], requires_grad=True)\n",
    "\n",
    "    # setting up the optimizer\n",
    "    parameters = [mean_coeff, lower_coeff, a, b]\n",
    "    optimizer = optim.Adam(parameters, learning_rate)\n",
    "\n",
    "    c_b = (1 / (2 * num_steps)) * torch.arange(1, 2 * num_steps + 1, 2)\n",
    "    c_b = c_b.view(-1, 1)\n",
    "    c_matrix = torch.cat((c_b, c_b ** 2, c_b ** 3), 1)\n",
    "\n",
    "    # training loop\n",
    "    train_losses, val_losses, epochs = [], [], []\n",
    "    train_len = len(train_loader)\n",
    "    val_len = len(val_loader)\n",
    "    print(\"Training starting !\")\n",
    "    for epoch_index in range(num_epochs):\n",
    "        print(\"Epoch no. \", epoch_index)\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            # mean and cov_mat batches calculations - mid point riemannian sum\n",
    "            a_matrix = torch.exp(torch.mm(c_matrix, -1 * a))\n",
    "            b_matrix = torch.exp(torch.mm(c_matrix, -1 * b))\n",
    "\n",
    "            # forming lower triangular matrix\n",
    "            lower_indices = torch.tril_indices(num_dim, num_dim)\n",
    "            lower_matrix = torch.zeros((num_dim, num_dim))\n",
    "            lower_matrix[lower_indices[0], lower_indices[1]] = lower_coeff\n",
    "\n",
    "            # defining batch multivariate normal distribution\n",
    "            mean = a_matrix * mean_coeff.repeat(\n",
    "                num_steps, 1\n",
    "            )  # batch mean along all values of c_b step varying from 0 to 1\n",
    "\n",
    "            cov_coeff = torch.matmul(\n",
    "                lower_matrix, torch.transpose(lower_matrix, 0, 1)\n",
    "            )  # cholesky decomposition\n",
    "            cov_mat = b_matrix[:, :, None] * cov_coeff\n",
    "            p = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "                mean, cov_mat\n",
    "            )  # batch normal distribution - takes all the distribution for different cb values together\n",
    "            optimizer.zero_grad()\n",
    "            batch_size = x.shape[0]\n",
    "            x_repeat = (\n",
    "                x.repeat(1, num_steps).view(-1, num_dim).view(batch_size, -1, num_dim)\n",
    "            )\n",
    "            # print(p.log_prob(x_repeat))\n",
    "            y_pred = (1 / num_steps) * torch.sum(\n",
    "                torch.exp(torch.add(p.log_prob(x_repeat), torch.log(N_total))), dim=1\n",
    "            )\n",
    "            loss = mean_squared_error(\n",
    "                y_pred, y\n",
    "            )  # loss function between predicted and true values\n",
    "            loss.backward(retain_graph=True)  # calculate gradients\n",
    "            optimizer.step()  # take a small step in the direction of gradient\n",
    "            train_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # scope of no gradient calculations\n",
    "                for x, y in val_loader:\n",
    "                    batch_size = x.shape[0]\n",
    "                    x_repeat = (\n",
    "                        x.repeat(1, num_steps)\n",
    "                        .view(-1, num_dim)\n",
    "                        .view(batch_size, -1, num_dim)\n",
    "                    )\n",
    "                    y_pred = (1 / num_steps) * torch.sum(\n",
    "                        torch.exp(torch.add(p.log_prob(x_repeat), torch.log(N_total))),\n",
    "                        dim=1,\n",
    "                    )\n",
    "                    loss = mean_squared_error(y_pred, y)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            train_losses.append(train_loss / train_len)\n",
    "            val_losses.append(val_loss / val_len)\n",
    "            epochs.append(epoch_index)\n",
    "            print(\n",
    "                \"Train loss: %.2f - Val loss: %.2f\"\n",
    "                % (train_loss / train_len, val_loss / val_len)\n",
    "            )\n",
    "\n",
    "    print(\"Training finished !\")\n",
    "\n",
    "    plt.plot(epochs, train_losses, color=\"red\")\n",
    "    plt.plot(epochs, val_losses, color=\"blue\")\n",
    "    plt.show()\n",
    "\n",
    "    return mean_coeff, lower_coeff, a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # load the dataset\n",
    "    X = np.load(\"data/x.npy\")\n",
    "    X[:, 1] = X[:, 1] * (1 / 1000)  # normalize the second column of the data\n",
    "    y = np.load(\"data/y.npy\").reshape(-1, 1)\n",
    "\n",
    "    # defining hyperparameters\n",
    "    batch_size = 100\n",
    "    validation_split = 0.2\n",
    "    num_dim = 2\n",
    "    num_epochs = 50\n",
    "    num_steps = 500\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    # prepare dataloader for optimization\n",
    "    train_loader, val_loader, N_total = data_preprocessing(\n",
    "        X, y, batch_size, validation_split\n",
    "    )\n",
    "\n",
    "    # training the model\n",
    "    mean_coeff, cov_coeff, a, b = fit(\n",
    "        train_loader, val_loader, N_total, num_dim, num_epochs, num_steps, learning_rate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()  # calling the main function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
